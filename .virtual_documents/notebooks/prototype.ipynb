import sys
import os
from transformers import AutoTokenizer
import torch

# Get project root 
project_root = os.getcwd()
src_path = os.path.join(project_root, 'src')

# Add src directory to sys.path
# Adapted from Taras Alenin's answer on StackOverflow at:
# https://stackoverflow.com/a/55623567
if src_path not in sys.path:
    sys.path.insert(0, src_path)

# Import custom modules
from prototype import SiameseBERT, ContrastiveLoss  # noqa: E402









#############################################################################
# CONSTANTS
#############################################################################

# all-MiniLM-L12-v2 pretrained model from Hugging Face:
# https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2
MODEL_NAME = "sentence-transformers/all-MiniLM-L12-v2"

# Learning rate of 2e-05 as per Ibrahim Et. Al (2023) [13:10]
# This is intentionally very small as we are fine-tuning a pre-trained model
LEARNING_RATE = 2e-05



#############################################################################
# MODEL INSTANTIATION
#############################################################################

# Instantiate custom Siamese SBERT model
model = SiameseBERT(MODEL_NAME)

# Instantiate custom contrastive loss function
# TODO: Consider implementing 'modified contrastive loss' from
# https://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf [18]
# and
# Tyo Et. Al (2021) [15]
loss_function = ContrastiveLoss(margin=1.0)

# Instantiate Adam optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)



#############################################################################
# DATA PREPROCESSING
#############################################################################

# Toy Dataset
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
toy_data = [
    ("This is a test sentence A.",
     "This is a test sentence B.",
     1),  # Similar pair
    ("This is a test sentence C.",
     "Sentence D is completely different.",
     0)  # Dissimilar pair
]


def tokenize_pair(text_a, text_b, tokenizer, max_length=128):
    """
    Tokenize two input sentences with a given tokenizer and max_length
    argument.

    :param text_a, text_b: The raw input text
    :type text_a, text_b: string
    :param tokenizer: The tokenizer
    :type tokenizer: transformers.models
    :param max_length: Max length in tokens that the output embedding should
        be. Defaults to 128
    :type max_length: int
    """

    # Truncate inputs that tokenize to tensors longer than `max_length`
    # Add special characters from BERT encoders, like [CLS] and [SEP]
    # TODO: perhaps revisit the above as these might add unhelpful noise
    # for our task
    tokens_a = tokenizer(text_a,
                         return_tensors="pt",
                         padding="max_length",
                         truncation=True,
                         max_length=max_length,
                         add_special_tokens=True)

    tokens_b = tokenizer(text_b,
                         return_tensors="pt",
                         padding="max_length",
                         truncation=True,
                         max_length=max_length,
                         add_special_tokens=True)

    return tokens_a, tokens_b


# Tokenize inputs
tokenized_pairs = []

for text_a, text_b, _ in toy_data:
    # Tokenize each pair of texts
    tokens_a, tokens_b = tokenize_pair(text_a, text_b, tokenizer)

    # Store the tokenized outputs
    tokenized_pairs.append((tokens_a, tokens_b))

# Create batched tensors

# Collate a tensor of row vectors containing indices into our pre-trained
# model's vocabulary, representing the sentences in position 0 (known works)
# ordered based on the ordering of the tokenized sentence.
known_author_input_ids = torch.cat([pair[0]['input_ids']
                                    for pair in tokenized_pairs])
# Do the same for tokenized sentences in position 1 (works to verify).
verification_text_input_ids = torch.cat([pair[1]['input_ids']
                                         for pair in tokenized_pairs])
# Collate the attention masks for sentences in position 0 similarly.
known_author_attention_mask = torch.cat([pair[0]['attention_mask']
                                         for pair in tokenized_pairs])
# Collate the attention masks for sentences in position 1 similarly.
verification_text_attention_mask = torch.cat([pair[1]['attention_mask']
                                              for pair in tokenized_pairs])

# Collate labels tensor, preserving ordering relative to input ids and
# attention masks.
labels = torch.tensor([label for _, _, label in toy_data])



#############################################################################
# FORWARD PASS
#############################################################################

# Forward pass through the model
u, v = model(known_author_input_ids, known_author_attention_mask,
             verification_text_input_ids, verification_text_attention_mask)



#############################################################################
# LOSS CALCULATION
#############################################################################

# Calculate loss
loss = loss_function(u, v, labels)
print(f"Loss: {loss.item()}")



#############################################################################
# BACKPROPAGATION
#############################################################################

# Clear out any existing gradients
optimizer.zero_grad()

# Backpropogation pass
loss.backward()

# Update weights using calculated gradients from Adam optimizer
optimizer.step()



#############################################################################
# INFERENCE
#############################################################################

# Set model to evaluation mode
model.eval()

# Run inference on the same toy training data
with torch.no_grad():  # Disable gradient calculation for inference
    # Forward pass through the model
    u_inference, v_inference = model(known_author_input_ids, known_author_attention_mask,
                                     verification_text_input_ids, verification_text_attention_mask)

    # Calculate cosine similarity between embeddings
    similarities = torch.nn.functional.cosine_similarity(u_inference,
                                                         v_inference)

    # Scale similarities from [-1,1] to [0,1] range
    scaled_similarities = (similarities + 1) / 2

    # Print results
    for i, (text_a, text_b, true_label) in enumerate(toy_data):
        print(f"\nPair {i+1}:")
        print(f"Text A: {text_a}")
        print(f"Text B: {text_b}")
        print(f"True Label: {true_label}")
        print(f"Similarity Score: {scaled_similarities[i]:.3f}")




