import sys
import os
from torch.utils.data import DataLoader
import torch
from definitions import ROOT_DIR, MODEL

# Add src directory to sys.path
# Adapted from Taras Alenin's answer on StackOverflow at:
# https://stackoverflow.com/a/55623567
src_path = os.path.join(ROOT_DIR, 'src')
if src_path not in sys.path:
    sys.path.insert(0, src_path)

# Import custom modules
from prototype import SiameseBERT, ContrastiveLoss  # noqa: E402
import dataset  # noqa: E402

# Try to use machines parallelism by setting env variable
os.environ["TOKENIZERS_PARALLELISM"] = "true"


#############################################################################
# CONSTANTS
#############################################################################

# Learning rate of 2e-05 as per Ibrahim Et. Al (2023) [13:10]
# This is intentionally very small as we are fine-tuning a pre-trained model
LEARNING_RATE = 2e-05



#############################################################################
# MODEL INSTANTIATION
#############################################################################

# Instantiate custom Siamese SBERT model
model = SiameseBERT(MODEL)

# Instantiate custom contrastive loss function
# TODO: Consider implementing 'modified contrastive loss' from
# https://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf [18]
# and
# Tyo Et. Al (2021) [15]
loss_function = ContrastiveLoss(margin=10.0)

# Instantiate Adam optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)



#############################################################################
# DATASET/LOADER INSTANTIATION
#############################################################################

train_ds = dataset.CustomDataset('data/cleaned', evaluate=False)



dataloader = DataLoader(
    train_ds,
    batch_size=32,
    shuffle=True,
    num_workers=4,
    collate_fn=dataset.collate_fn
)


#############################################################################
# TRAINING
#############################################################################

# Collate labels tensor, preserving ordering relative to input ids and
# attention masks.
train_labels = torch.Tensor([])
losses = []

for batch_a, batch_b, labels in dataloader:
    train_labels = torch.cat([train_labels, labels])
    u, v = model(
        batch_a['input_ids'], 
        batch_a['attention_mask'],
        batch_b['input_ids'], 
        batch_b['attention_mask']
    )
    loss = loss_function(u, v, labels)
    losses.append(loss)

    # Clear out any existing gradients
    optimizer.zero_grad()

    # Backpropogation pass
    loss.backward()

    # Update weights using calculated gradients from Adam optimizer
    optimizer.step()


import matplotlib.pyplot as plt
import torch

# Convert list of tensors to regular numbers
losses_plain = [tensor.item() for tensor in losses]

# Create the plot
plt.figure(figsize=(10, 6))
plt.plot(losses_plain, 'b-', label='Training Loss')
plt.grid(True)
plt.xlabel('Batch')
plt.ylabel('Loss')
plt.title('Training Loss over Time')
plt.legend()

plt.yscale('log')
plt.grid(True, which="both", ls="-", alpha=0.2)

# Show the plot
plt.tight_layout()
plt.show()


#############################################################################
# DATASET/LOADER INSTANTIATION
#############################################################################

eval_ds = dataset.CustomDataset('data/cleaned', evaluate=True)



eval_dataloader = DataLoader(
    eval_ds,
    batch_size=32,
    shuffle=True,
    num_workers=4,
    collate_fn=dataset.collate_fn
)


#############################################################################
# INFERENCE
#############################################################################

# Set model to evaluation mode
model.eval()

# Collate labels tensor, preserving ordering relative to input ids and
# attention masks.
ground_truths = torch.Tensor([])
pred_probs = torch.Tensor([])

# Run inference
with torch.no_grad():  # Disable gradient calculation for inference
    for batch_a, batch_b, labels in eval_dataloader:
        ground_truths = torch.cat([ground_truths, labels])
        # Forward pass through the model
        u_inference, v_inference = model(batch_a['input_ids'], 
                                         batch_a['attention_mask'],
                                         batch_b['input_ids'], 
                                         batch_b['attention_mask'])

        # Calculate cosine similarity between embeddings
        similarities = torch.nn.functional.cosine_similarity(u_inference,
                                                             v_inference)

        # Scale similarities from [-1,1] to [0,1] range
        scaled_similarities = (similarities + 1) / 2

        # Add scaled similarities to pred_probs
        pred_probs = torch.cat([pred_probs, scaled_similarities])



preds = pred_probs.round()


# Adapted from:
# https://www.w3schools.com/python/python_ml_confusion_matrix.asp
# Accessed 2024-12-14
import matplotlib.pyplot as plt
import numpy
from sklearn import metrics

confusion_matrix = metrics.confusion_matrix(ground_truths, preds)

cm_display = metrics.ConfusionMatrixDisplay(
    confusion_matrix = confusion_matrix,
    display_labels = [0, 1])

cm_display.plot()
plt.show()


ground_truths[0:10]


pred_probs[0:10]


pred_probs.mean()


preds[0:10]


ground_truths.type(torch.int64).bincount()


preds.type(torch.int64).bincount()


c = 0
for i in range(64):
    for j in range(i + 1, 64):
        c += 1
print(c)


len(train_ds._A_chunks) + len(train_ds._notA_chunks)


2016/32



