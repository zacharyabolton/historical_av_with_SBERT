from multiprocessing import Pool
import defs


def run_parallel(dataset, k_folds, k_td_values, devices):
    results = []
    with Pool(len(devices)) as pool:
        # Create verifier instances for each device
        tasks = [(k_td, fold, device) for k_td in k_td_values 
                                    for fold in range(k_folds)
                                    for device in devices]
        results = pool.starmap(defs.train_worker, tasks)
    return results


run_parallel('foo', 3, [0, 0.5, 1], [1,2,3])


import defs
from multiprocessing import Pool
import time

# Test sequential
start_time = time.time()
results_seq = [defs.train_worker_2(i) for i in range(3)]
seq_time = time.time() - start_time
print(f"Sequential took: {seq_time:.2f} seconds")

# Test parallel
start_time = time.time()
with Pool(3) as p:
    results_par = p.map(defs.train_worker_2, range(3))
par_time = time.time() - start_time
print(f"Parallel took: {par_time:.2f} seconds")

print(f"Speedup factor: {seq_time/par_time:.2f}x")


import os
import time
from multiprocessing import Pool
import defs

os.environ["TOKENIZERS_PARALLELISM"] = "false"

def run_benchmark():
    # Test sequential
    print("Running sequential training...")
    start_time = time.time()
    results = [defs.train_fold(i) for i in range(3)]
    seq_time = time.time() - start_time
    print(f"Sequential took: {seq_time:.2f} seconds")
    
    # Test parallel
    print("\nRunning parallel training...")
    start_time = time.time()
    with Pool(3) as p:
        results = p.map(defs.train_fold, range(3))
    par_time = time.time() - start_time
    print(f"Parallel took: {par_time:.2f} seconds")
    print(f"Speedup factor: {seq_time/par_time:.2f}x")

run_benchmark()


# @inproceedings{reimers-2019-sentence-bert,
#   title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
#   author = "Reimers, Nils and Gurevych, Iryna",
#   booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
#   month = "11",
#   year = "2019",
#   publisher = "Association for Computational Linguistics",
#   url = "https://arxiv.org/abs/1908.10084",
# }

from sentence_transformers import SentenceTransformer

# 1. Load a pretrained Sentence Transformer model
model = SentenceTransformer("all-MiniLM-L6-v2")

# The sentences to encode
sentences = [
    "The weather is lovely today.",
    "It's so sunny outside!",
    "He drove to the stadium.",
]

# 2. Calculate embeddings by calling model.encode()
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# 3. Calculate the embedding similarities
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# tensor([[1.0000, 0.6660, 0.1046],
#         [0.6660, 1.0000, 0.1411],
#         [0.1046, 0.1411, 1.0000]])
