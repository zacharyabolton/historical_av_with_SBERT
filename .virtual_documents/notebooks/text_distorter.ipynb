from nltk.probability import FreqDist
import os
import re


def get_W_k(data_dir, canonical_class_labels):

    # Setup the NLTK FeqDist object to calculate the occurances of all words
    # in the corpus.
    fdist = FreqDist()

    # Loop through the sub-directories to collect samples in each input
    # class.
    for label in canonical_class_labels:
        # Create the path to the class samples.
        class_dir = os.path.join(data_dir, label)
        # Loop through each file in the class directory.
        for file in os.listdir(class_dir):
            # Only process `.txt` files.
            if file.endswith('.txt'):
                # Open the file.
                with open(os.path.join(class_dir, file)) as f:
                    # Read in the file's contents.
                    text = f.read()
                    # 'Tokenize' the text into words by splitting on
                    # whitespace.
                    words = text.split()
                    # Loop through all the words in the text.
                    for word in words:
                        # Add or increment each word occurence to the
                        # frequency distribution object.
                        fdist[word.lower()] += 1

    print('unique words', fdist.B())
    print('hapax legomena', len(fdist.hapaxes()))
    print('proportion', len(fdist.hapaxes())/fdist.B())


# Set the data directory path
data_dir = '../data/preprocessed'
# Set the class conical names
canonical_class_names = ['notA', 'A', 'U']

# Get the top `k` most frequent words from the entire corpus.
get_W_k(data_dir, canonical_class_names)







