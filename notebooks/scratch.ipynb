{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bee54d53-9b8e-4f2a-a919-8768cfb4fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from definitions import ROOT_DIR, MODEL\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Get the relevant directories\n",
    "src_path = os.path.join(ROOT_DIR, 'src')\n",
    "\n",
    "# Add src directory to sys.path\n",
    "# Adapted from Taras Alenin's answer on StackOverflow at:\n",
    "# https://stackoverflow.com/a/55623567\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Import my custom modules\n",
    "import dataset  # noqa: E402\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a41983e-63e2-45cf-a53b-618cc6b8c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# DATA PREPROCESSING\n",
    "#############################################################################\n",
    "\n",
    "# Toy Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "toy_data = [\n",
    "    (\"This is a test sentence A.\",\n",
    "     \"This is a test sentence B.\",\n",
    "     1),  # Similar pair\n",
    "    (\"This is a test sentence C.\",\n",
    "     \"Sentence D is completely different.\",\n",
    "     0)  # Dissimilar pair\n",
    "]\n",
    "\n",
    "\n",
    "def tokenize_pair(text_a, text_b, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize two input sentences with a given tokenizer and max_length\n",
    "    argument.\n",
    "\n",
    "    :param text_a, text_b: The raw input text\n",
    "    :type text_a, text_b: string\n",
    "    :param tokenizer: The tokenizer\n",
    "    :type tokenizer: transformers.models\n",
    "    :param max_length: Max length in tokens that the output embedding should\n",
    "        be. Defaults to 128\n",
    "    :type max_length: int\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate inputs that tokenize to tensors longer than `max_length`\n",
    "    # Add special characters from BERT encoders, like [CLS] and [SEP]\n",
    "    # TODO: perhaps revisit the above as these might add unhelpful noise\n",
    "    # for our task\n",
    "    tokens_a = tokenizer(text_a,\n",
    "                         return_tensors=\"pt\",\n",
    "                         padding=\"max_length\",\n",
    "                         truncation=True,\n",
    "                         max_length=max_length,\n",
    "                         add_special_tokens=True)\n",
    "\n",
    "    tokens_b = tokenizer(text_b,\n",
    "                         return_tensors=\"pt\",\n",
    "                         padding=\"max_length\",\n",
    "                         truncation=True,\n",
    "                         max_length=max_length,\n",
    "                         add_special_tokens=True)\n",
    "\n",
    "    return tokens_a, tokens_b\n",
    "\n",
    "\n",
    "# Tokenize inputs\n",
    "tokenized_pairs = []\n",
    "\n",
    "for text_a, text_b, _ in toy_data:\n",
    "    # Tokenize each pair of texts\n",
    "    tokens_a, tokens_b = tokenize_pair(text_a, text_b, tokenizer)\n",
    "\n",
    "    # Store the tokenized outputs\n",
    "    tokenized_pairs.append((tokens_a, tokens_b))\n",
    "\n",
    "# Create batched tensors\n",
    "\n",
    "# Collate a tensor of row vectors containing indices into our pre-trained\n",
    "# model's vocabulary, representing the sentences in position 0 (known works)\n",
    "# ordered based on the ordering of the tokenized sentence.\n",
    "known_author_input_ids = torch.cat([pair[0]['input_ids']\n",
    "                                    for pair in tokenized_pairs])\n",
    "# Do the same for tokenized sentences in position 1 (works to verify).\n",
    "verification_text_input_ids = torch.cat([pair[1]['input_ids']\n",
    "                                         for pair in tokenized_pairs])\n",
    "# Collate the attention masks for sentences in position 0 similarly.\n",
    "known_author_attention_mask = torch.cat([pair[0]['attention_mask']\n",
    "                                         for pair in tokenized_pairs])\n",
    "# Collate the attention masks for sentences in position 1 similarly.\n",
    "verification_text_attention_mask = torch.cat([pair[1]['attention_mask']\n",
    "                                              for pair in tokenized_pairs])\n",
    "\n",
    "# Collate labels tensor, preserving ordering relative to input ids and\n",
    "# attention masks.\n",
    "labels = torch.tensor([label for _, _, label in toy_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67cbced5-023e-4975-b747-45100f7f1722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = dataset.CustomDataset('data/test')\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51209f76-3f29-46d0-870d-a980bcc2a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    ds,\n",
    "    batch_size=6,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dfffb9b-7977-4096-ba7d-27687872034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': tensor([[[  101, 29379, 29379,   102]],\n",
      "\n",
      "        [[  101,  1040,  1040,   102]],\n",
      "\n",
      "        [[  101, 29379, 29379,   102]],\n",
      "\n",
      "        [[  101,  3347,  3347,   102]],\n",
      "\n",
      "        [[  101,  3347,  3347,   102]],\n",
      "\n",
      "        [[  101, 29379, 29379,   102]]]), 'attention_mask': tensor([[[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]]])}, {'input_ids': tensor([[[ 101, 1040, 1040,  102]],\n",
      "\n",
      "        [[ 101, 1038, 1038,  102]],\n",
      "\n",
      "        [[ 101, 8670, 2480,  102]],\n",
      "\n",
      "        [[ 101, 1040, 1040,  102]],\n",
      "\n",
      "        [[ 101, 1037, 1037,  102]],\n",
      "\n",
      "        [[ 101, 3347, 3347,  102]]]), 'attention_mask': tensor([[[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]]])}, tensor([1, 0, 0, 1, 1, 1])]\n",
      "[{'input_ids': tensor([[[  101,  1037,  1037,   102]],\n",
      "\n",
      "        [[  101,  1037,  1037,   102]],\n",
      "\n",
      "        [[  101,  1040,  1040,   102]],\n",
      "\n",
      "        [[  101,  1037,  1037,   102]],\n",
      "\n",
      "        [[  101, 29379, 29379,   102]],\n",
      "\n",
      "        [[  101,  1037,  1037,   102]]]), 'attention_mask': tensor([[[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]]])}, {'input_ids': tensor([[[  101, 24209,  2595,   102]],\n",
      "\n",
      "        [[  101,  8670,  2480,   102]],\n",
      "\n",
      "        [[  101, 24209,  2595,   102]],\n",
      "\n",
      "        [[  101,  1040,  1040,   102]],\n",
      "\n",
      "        [[  101,  1037,  1037,   102]],\n",
      "\n",
      "        [[  101,  1038,  1038,   102]]]), 'attention_mask': tensor([[[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1]]])}, tensor([0, 0, 0, 1, 1, 0])]\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9821b8-d11b-4ae8-973e-bf2239916ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Historical AV with SBERT Project Specific Kernel",
   "language": "python",
   "name": "historical_av_with_sbert-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
