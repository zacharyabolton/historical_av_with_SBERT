{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f992abdb-c3f8-467a-81b2-327c71d15103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from constants import MODEL\n",
    "from siamese_sbert import SiameseSBERT\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1000524c-0b5d-4844-9940-97aa498e31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelization/Concurency\n",
    "# Use CUDA if available, else use MPS if available. Fallback is CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                      else (\n",
    "                        \"mps\"\n",
    "                        if torch.backends.mps.is_available()\n",
    "                        else \"cpu\"\n",
    "                      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "959f4da9-1d92-4bec-ac00-7747836ceb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(checkpoint_path, text_pair, tokenizer, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load a saved model and run inference on a text pair.\n",
    "\n",
    "    :param checkpoint_path: Path to the saved checkpoint\n",
    "    :type checkpoint_path: str\n",
    "\n",
    "    :param text_pair: Tuple of (text1, text2) to compare\n",
    "    :type text_pair: tuple\n",
    "\n",
    "    :param tokenizer: The BERT tokenizer\n",
    "    :param tokenizer: transformers.Autotokenizer\n",
    "\n",
    "    :param device: Device to run inference on\n",
    "    :type device: str\n",
    "\n",
    "    :returns: Similarity score between 0-1\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = SiameseSBERT(MODEL, device).to(device)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize inputs\n",
    "    text1, text2 = text_pair\n",
    "    tokens1 = tokenizer(text1,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=512).to(device)\n",
    "    tokens2 = tokenizer(text2,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=512).to(device)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        embeddings1, embeddings2 = model(\n",
    "            tokens1['input_ids'],\n",
    "            tokens1['attention_mask'],\n",
    "            tokens2['input_ids'],\n",
    "            tokens2['attention_mask']\n",
    "        )\n",
    "\n",
    "        # Calculate similarity\n",
    "        similarity = F.cosine_similarity(embeddings1, embeddings2)\n",
    "        # Scale from [-1,1] to [0,1]\n",
    "        scaled_similarity = (similarity + 1) / 2\n",
    "\n",
    "    return scaled_similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e60f9932-c8b5-4c3f-ac2e-e64d3b81dfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l2/g5pq_4kj4x7fxdjx_zj7zrb00000gn/T/ipykernel_97578/4182380832.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.9587\n",
      "Predicted same author: True\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Load and run inference\n",
    "checkpoint_path = \"/Users/zacbolton/dev/BSc/FP/historical_av_with_SBERT/model_out/model_saving_exp_2/undistorted/model_saving_exp_2_fold_1_epoch_0.pt\"\n",
    "text1 = \"First piece of text to compare\"\n",
    "text2 = \"Second piece of text to compare\"\n",
    "\n",
    "similarity = run_inference(checkpoint_path,\n",
    "                           (text1, text2),\n",
    "                           tokenizer,\n",
    "                           device=device)\n",
    "\n",
    "print(f\"Similarity score: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff83243-4b47-4c60-aa34-519201e42777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Historical AV with SBERT Project Specific Kernel",
   "language": "python",
   "name": "historical_av_with_sbert-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
