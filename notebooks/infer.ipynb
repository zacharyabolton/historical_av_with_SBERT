{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f992abdb-c3f8-467a-81b2-327c71d15103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from constants import MODEL\n",
    "from siamese_sbert import SiameseSBERT\n",
    "from lila_dataset import LILADataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1000524c-0b5d-4844-9940-97aa498e31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelization/Concurency\n",
    "# Use CUDA if available, else use MPS if available. Fallback is CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                      else (\n",
    "                        \"mps\"\n",
    "                        if torch.backends.mps.is_available()\n",
    "                        else \"cpu\"\n",
    "                      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b86846-c8e4-44d3-836b-b1b6246d21cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zacbolton/dev/BSc/FP/historical_av_with_SBERT/src/lila_dataset.py:119: UserWarning: Running letters inference, so parameters `num_pairs` and `num_folds` will not be applied\n",
      "  warnings.warn(\"Running letters inference, so parameters\"\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21972 > 512). Running this sequence through the model will result in indexing errors\n",
      "/var/folders/l2/g5pq_4kj4x7fxdjx_zj7zrb00000gn/T/ipykernel_41482/3394478073.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "/Users/zacbolton/dev/BSc/FP/historical_av_with_SBERT/src/lila_dataset.py:119: UserWarning: Running letters inference, so parameters `num_pairs` and `num_folds` will not be applied\n",
      "  warnings.warn(\"Running letters inference, so parameters\"\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (58898 > 512). Running this sequence through the model will result in indexing errors\n",
      "/var/folders/l2/g5pq_4kj4x7fxdjx_zj7zrb00000gn/T/ipykernel_41482/3394478073.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "/Users/zacbolton/dev/BSc/FP/historical_av_with_SBERT/src/lila_dataset.py:119: UserWarning: Running letters inference, so parameters `num_pairs` and `num_folds` will not be applied\n",
      "  warnings.warn(\"Running letters inference, so parameters\"\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (24306 > 512). Running this sequence through the model will result in indexing errors\n",
      "/var/folders/l2/g5pq_4kj4x7fxdjx_zj7zrb00000gn/T/ipykernel_41482/3394478073.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "/Users/zacbolton/dev/BSc/FP/historical_av_with_SBERT/src/lila_dataset.py:119: UserWarning: Running letters inference, so parameters `num_pairs` and `num_folds` will not be applied\n",
      "  warnings.warn(\"Running letters inference, so parameters\"\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37976 > 512). Running this sequence through the model will result in indexing errors\n",
      "/var/folders/l2/g5pq_4kj4x7fxdjx_zj7zrb00000gn/T/ipykernel_41482/3394478073.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '../data/normalized'\n",
    "undistorted_path = os.path.join(dataset_path, 'undistorted')\n",
    "assert os.path.exists(undistorted_path)\n",
    "metadata_path = os.path.join(dataset_path, 'metadata.csv')\n",
    "assert os.path.exists(metadata_path)\n",
    "\n",
    "# Create list to store all views to process\n",
    "views = [undistorted_path]\n",
    "\n",
    "for view_dir in os.listdir(dataset_path):\n",
    "    view_path = os.path.join(dataset_path, view_dir)\n",
    "    if view_dir != 'undistorted' and\\\n",
    "       os.path.isdir(view_path) and\\\n",
    "       view_dir[0] != '.':\n",
    "        assert view_dir[:8] == 'DV-SA-k-' or view_dir[:8] == 'DV-MA-k-'\n",
    "        views.append(view_path)\n",
    "\n",
    "view_preds = {}\n",
    "for view_path in views:\n",
    "    # Get simple view string\n",
    "    # Adapted from:\n",
    "    # https://stackoverflow.com/a/3925147\n",
    "    view = os.path.basename(os.path.normpath(view_path))\n",
    "    # Add an entry to view_preds to store predictions for this\n",
    "    # particular view\n",
    "    view_preds[view] = []\n",
    "\n",
    "    # Reset any existing splits\n",
    "    LILADataset.reset_splits()\n",
    "\n",
    "    # Instantiate the full LILA dataset\n",
    "    inference_dataset = LILADataset(view_path,\n",
    "                                    metadata_path,\n",
    "                                    cnk_size=512,\n",
    "                                    num_pairs=0,\n",
    "                                    seed=0,\n",
    "                                    letters=True)\n",
    "\n",
    "    # Load model\n",
    "    model = SiameseSBERT(MODEL, device).to(device)\n",
    "    checkpoint_path = (\"/Users/zacbolton/dev/BSc/FP/\"\n",
    "                       \"historical_av_with_SBERT/saved_experiments/\"\n",
    "                       f\"full_run/{view}/\"\n",
    "                       \"full_run_fold_4_epoch_2.pt\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    for pair in inference_dataset._pairs:\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            # Move input tensors to device\n",
    "            input_ids_1 = pair[0]['input_ids'].to(device)\n",
    "            attention_mask_1 = pair[0]['attention_mask'].to(device)\n",
    "            input_ids_2 = pair[1]['input_ids'].to(device)\n",
    "            attention_mask_2 = pair[1]['attention_mask'].to(device)\n",
    "\n",
    "            embeddings1, embeddings2 = model(\n",
    "                input_ids_1,\n",
    "                attention_mask_1,\n",
    "                input_ids_2,\n",
    "                attention_mask_2\n",
    "            )\n",
    "\n",
    "            # Calculate similarity\n",
    "            similarity = F.cosine_similarity(embeddings1, embeddings2)\n",
    "            # Scale from [-1,1] to [0,1]\n",
    "            scaled_similarity = (similarity + 1) / 2\n",
    "\n",
    "        view_preds[view].append(scaled_similarity.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64495c85-569e-4a84-b666-c7415216a700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VIEW undistorted\n",
      "\n",
      "Mumber of predictions: 820\n",
      "Mean:                  0.7086946412920951\n",
      "STD:                   0.29838421972626655\n",
      "Median:                0.8612443208694458\n",
      "\n",
      "Same-author:           0.7439024390243902\n",
      "Different-author:      0.2475609756097561\n",
      "Undecided:             0.00853658536585366\n",
      "\n",
      "Quartiles:             [0.45598616 0.86124432 0.92561182]\n",
      "Min:                   0.05405956506729126\n",
      "Max:                   0.9899935126304626\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "VIEW DV-MA-k-300\n",
      "\n",
      "Mumber of predictions: 2160\n",
      "Mean:                  0.5635489236287496\n",
      "STD:                   0.271862259741844\n",
      "Median:                0.6489334106445312\n",
      "\n",
      "Same-author:           0.5777777777777777\n",
      "Different-author:      0.375\n",
      "Undecided:             0.04722222222222222\n",
      "\n",
      "Quartiles:             [0.29706125 0.64893341 0.81157312]\n",
      "Min:                   0.044049471616744995\n",
      "Max:                   0.9589341282844543\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "VIEW DV-MA-k-20000\n",
      "\n",
      "Mumber of predictions: 880\n",
      "Mean:                  0.6612847168675878\n",
      "STD:                   0.2955986602665388\n",
      "Median:                0.7814838886260986\n",
      "\n",
      "Same-author:           0.6818181818181818\n",
      "Different-author:      0.2761363636363636\n",
      "Undecided:             0.042045454545454546\n",
      "\n",
      "Quartiles:             [0.39835116 0.78148389 0.90544897]\n",
      "Min:                   0.04066658020019531\n",
      "Max:                   0.9850629568099976\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "VIEW DV-MA-k-3000\n",
      "\n",
      "Mumber of predictions: 1360\n",
      "Mean:                  0.5977668894126135\n",
      "STD:                   0.33395606897889063\n",
      "Median:                0.7258614003658295\n",
      "\n",
      "Same-author:           0.6014705882352941\n",
      "Different-author:      0.34705882352941175\n",
      "Undecided:             0.051470588235294115\n",
      "\n",
      "Quartiles:             [0.20372868 0.7258614  0.90362625]\n",
      "Min:                   0.029700934886932373\n",
      "Max:                   0.9901821613311768\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p1=0.45\n",
    "p2=0.54\n",
    "\n",
    "for key in view_preds:\n",
    "    preds = np.array(view_preds[key])\n",
    "\n",
    "    print(f\"\"\"\n",
    "VIEW {key}\n",
    "\n",
    "Mumber of predictions: {len(preds)}\n",
    "Mean:                  {np.mean(preds)}\n",
    "STD:                   {np.std(preds)}\n",
    "Median:                {np.median(preds)}\n",
    "\n",
    "Same-author:           {np.mean(preds > p2)}\n",
    "Different-author:      {np.mean(preds < p1)}\n",
    "Undecided:             {np.mean((preds >= p1) & (preds <= p2))}\n",
    "\n",
    "Quartiles:             {np.percentile(preds, [25, 50, 75])}\n",
    "Min:                   {np.min(preds)}\n",
    "Max:                   {np.max(preds)}\n",
    "\n",
    "---\\n\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Historical AV with SBERT Project Specific Kernel",
   "language": "python",
   "name": "historical_av_with_sbert-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
