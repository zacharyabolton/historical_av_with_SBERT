{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c8ab19-13d4-41af-b4ea-aa9c308985e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Get project root \n",
    "project_root = os.getcwd()\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "\n",
    "# Add src directory to sys.path\n",
    "# Adapted from Taras Alenin's answer on StackOverflow at:\n",
    "# https://stackoverflow.com/a/55623567\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Import custom modules\n",
    "from prototype import SiameseBERT, ContrastiveLoss  # noqa: E402\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641351a4-a324-4a43-9cf9-0102cb6600af",
   "metadata": {},
   "source": [
    "# Prototype\n",
    "\n",
    "This demonstrates the Siamese SBERT model architecture for authorship verification (AV). For more details on the architecture and implementation, see the module docstring in `/src/prototype.py`.\n",
    "\n",
    "Below is a modified version of my model architecture diagram showing the prototype architecture at training (left) and inference (right). It notes that the prototype is intended for a single forward and backward pass on a micro-batch of two sample pairs. It strikes out references to chunking, as that is not handled for this prototype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dbdd6e-ed2d-4377-8566-5b22a1cf9f4f",
   "metadata": {},
   "source": [
    "_Figure 1_: **_Prototype_ architecture**\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph \"SBERT Training\"\n",
    "        pab1[\"Pair (A, B),\\n<s style='text-decoration-thickness: 4px; text-decoration-color: red'>chunk len = 256</s>\"] --> chp1[\"<s style='text-decoration-thickness: 4px; text-decoration-color: red'>Chunking Pair</s>\"]\n",
    "        chp1 --> np1[\"<s style='text-decoration-thickness: 4px; text-decoration-color: red'>N</s> 2 pairs\"]\n",
    "        np1 --> pnon1[\"Pair n out of <s style='text-decoration-thickness: 4px; text-decoration-color: red'>N</s> 2\"]\n",
    "        pnon1 --> can1[\"<s style='text-decoration-thickness: 4px; text-decoration-color: red'>Chunk</s> A.n\"]\n",
    "        pnon1 --> can2[\"<s style='text-decoration-thickness: 4px; text-decoration-color: red'>Chunk</s> B.n\"]\n",
    "        can1 --> bert1[BERT]\n",
    "        can2 --> bert2[BERT]\n",
    "        bert1 --> pool1[Mean Pooling]\n",
    "        bert2 --> pool2[Mean Pooling]\n",
    "        pool1 --> u1[U]\n",
    "        pool2 --> v1[V]\n",
    "        u1 --> contrastive[Contrastive Loss]\n",
    "        v1 --> contrastive\n",
    "    end\n",
    "\n",
    "    subgraph \"SBERT at Inference\"\n",
    "        pab2[\"Pair (A, B),\\n<s style='text-decoration-thickness: 4px; text-decoration-color: red'>chunk len = 256</s>\"] --> chp2[\"<s style='text-decoration-thickness: 4px; text-decoration-color: red'>Chunking Pair</s>\"]\n",
    "        chp2 --> np2[\"<s style='text-decoration-thickness: 4px; text-decoration-color: red'>N</s> 2 pairs\"]\n",
    "        np2 --> pnon2[\"Pair n out of <s style='text-decoration-thickness: 4px; text-decoration-color: red'>N</s> 2\"]\n",
    "        pnon2 --> can3[\"<s style='text-decoration-thickness: 4px; text-decoration-color: red'>Chunk</s> A.n\"]\n",
    "        pnon2 --> can4[\"<s style='text-decoration-thickness: 4px; text-decoration-color: red'>Chunk</s> B.n\"]\n",
    "        can3 --> bert3[BERT]\n",
    "        can4 --> bert4[BERT]\n",
    "        bert3 --> pool3[Mean Pooling]\n",
    "        bert4 --> pool4[Mean Pooling]\n",
    "        pool3 --> u2[U]\n",
    "        pool4 --> v2[V]\n",
    "        u2 --> cos[\"Cosine-Similarity(U,V)\"]\n",
    "        v2 --> cos\n",
    "        cos --> sim[\"Similarity Score n\"]\n",
    "    end\n",
    "    \n",
    "    note[\"For one micro-batch of 2 sample pairs\"] --- pab1\n",
    "    note --- pab2\n",
    "    \n",
    "    style contrastive fill:#9999ff\n",
    "    style contrastive color:#000000\n",
    "    style cos fill:#ff9999\n",
    "    style cos color:#000000\n",
    "    style sim fill:#ff9999\n",
    "    style sim color:#000000\n",
    "    style pab1 fill:#999999\n",
    "    style pab1 color:#000000\n",
    "    style pab2 fill:#999999\n",
    "    style pab2 color:#000000\n",
    "    linkStyle 27,28 stroke:#999999,stroke-width:1px,stroke-dasharray: 5 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb1092d5-c8c4-4063-b8c6-5bf9619483b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# CONSTANTS\n",
    "#############################################################################\n",
    "\n",
    "# all-MiniLM-L12-v2 pretrained model from Hugging Face:\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "\n",
    "# Learning rate of 2e-05 as per Ibrahim Et. Al (2023) [13:10]\n",
    "# This is intentionally very small as we are fine-tuning a pre-trained model\n",
    "LEARNING_RATE = 2e-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c16b9d-f3db-46dc-a4b9-18977aa8a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# MODEL INSTANTIATION\n",
    "#############################################################################\n",
    "\n",
    "# Instantiate custom Siamese SBERT model\n",
    "model = SiameseBERT(MODEL_NAME)\n",
    "\n",
    "# Instantiate custom contrastive loss function\n",
    "# TODO: Consider implementing 'modified contrastive loss' from\n",
    "# https://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf [18]\n",
    "# and\n",
    "# Tyo Et. Al (2021) [15]\n",
    "loss_function = ContrastiveLoss(margin=1.0)\n",
    "\n",
    "# Instantiate Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd6a70ba-ae10-413f-bdef-c0cf3294f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# DATA PREPROCESSING\n",
    "#############################################################################\n",
    "\n",
    "# Toy Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "toy_data = [\n",
    "    (\"This is a test sentence A.\",\n",
    "     \"This is a test sentence B.\",\n",
    "     1),  # Similar pair\n",
    "    (\"This is a test sentence C.\",\n",
    "     \"Sentence D is completely different.\",\n",
    "     0)  # Dissimilar pair\n",
    "]\n",
    "\n",
    "\n",
    "def tokenize_pair(text_a, text_b, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize two input sentences with a given tokenizer and max_length\n",
    "    argument.\n",
    "\n",
    "    :param text_a, text_b: The raw input text\n",
    "    :type text_a, text_b: string\n",
    "    :param tokenizer: The tokenizer\n",
    "    :type tokenizer: transformers.models\n",
    "    :param max_length: Max length in tokens that the output embedding should\n",
    "        be. Defaults to 128\n",
    "    :type max_length: int\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate inputs that tokenize to tensors longer than `max_length`\n",
    "    # Add special characters from BERT encoders, like [CLS] and [SEP]\n",
    "    # TODO: perhaps revisit the above as these might add unhelpful noise\n",
    "    # for our task\n",
    "    tokens_a = tokenizer(text_a,\n",
    "                         return_tensors=\"pt\",\n",
    "                         padding=\"max_length\",\n",
    "                         truncation=True,\n",
    "                         max_length=max_length,\n",
    "                         add_special_tokens=True)\n",
    "\n",
    "    tokens_b = tokenizer(text_b,\n",
    "                         return_tensors=\"pt\",\n",
    "                         padding=\"max_length\",\n",
    "                         truncation=True,\n",
    "                         max_length=max_length,\n",
    "                         add_special_tokens=True)\n",
    "\n",
    "    return tokens_a, tokens_b\n",
    "\n",
    "\n",
    "# Tokenize inputs\n",
    "tokenized_pairs = []\n",
    "\n",
    "for text_a, text_b, _ in toy_data:\n",
    "    # Tokenize each pair of texts\n",
    "    tokens_a, tokens_b = tokenize_pair(text_a, text_b, tokenizer)\n",
    "\n",
    "    # Store the tokenized outputs\n",
    "    tokenized_pairs.append((tokens_a, tokens_b))\n",
    "\n",
    "# Create batched tensors\n",
    "\n",
    "# Collate a tensor of row vectors containing indices into our pre-trained\n",
    "# model's vocabulary, representing the sentences in position 0 (known works)\n",
    "# ordered based on the ordering of the tokenized sentence.\n",
    "known_author_input_ids = torch.cat([pair[0]['input_ids']\n",
    "                                    for pair in tokenized_pairs])\n",
    "# Do the same for tokenized sentences in position 1 (works to verify).\n",
    "verification_text_input_ids = torch.cat([pair[1]['input_ids']\n",
    "                                         for pair in tokenized_pairs])\n",
    "# Collate the attention masks for sentences in position 0 similarly.\n",
    "known_author_attention_mask = torch.cat([pair[0]['attention_mask']\n",
    "                                         for pair in tokenized_pairs])\n",
    "# Collate the attention masks for sentences in position 1 similarly.\n",
    "verification_text_attention_mask = torch.cat([pair[1]['attention_mask']\n",
    "                                              for pair in tokenized_pairs])\n",
    "\n",
    "# Collate labels tensor, preserving ordering relative to input ids and\n",
    "# attention masks.\n",
    "labels = torch.tensor([label for _, _, label in toy_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6f0b10-8af3-4943-8ea9-82368d2e5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# FORWARD PASS\n",
    "#############################################################################\n",
    "\n",
    "# Forward pass through the model\n",
    "u, v = model(known_author_input_ids, known_author_attention_mask,\n",
    "             verification_text_input_ids, verification_text_attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46cad900-6449-4458-bd2c-2699d53184d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3593752980232239\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# LOSS CALCULATION\n",
    "#############################################################################\n",
    "\n",
    "# Calculate loss\n",
    "loss = loss_function(u, v, labels)\n",
    "print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69349b10-b925-43f3-8e64-f9f4749f814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# BACKPROPAGATION\n",
    "#############################################################################\n",
    "\n",
    "# Clear out any existing gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Backpropogation pass\n",
    "loss.backward()\n",
    "\n",
    "# Update weights using calculated gradients from Adam optimizer\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec78d9ae-bcca-438f-898b-1235ead7c81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pair 1:\n",
      "Text A: This is a test sentence A.\n",
      "Text B: This is a test sentence B.\n",
      "True Label: 1\n",
      "Similarity Score: 0.998\n",
      "\n",
      "Pair 2:\n",
      "Text A: This is a test sentence C.\n",
      "Text B: Sentence D is completely different.\n",
      "True Label: 0\n",
      "Similarity Score: 0.797\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# INFERENCE\n",
    "#############################################################################\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Run inference on the same toy training data\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    # Forward pass through the model\n",
    "    u_inference, v_inference = model(known_author_input_ids, known_author_attention_mask,\n",
    "                                     verification_text_input_ids, verification_text_attention_mask)\n",
    "\n",
    "    # Calculate cosine similarity between embeddings\n",
    "    similarities = torch.nn.functional.cosine_similarity(u_inference,\n",
    "                                                         v_inference)\n",
    "\n",
    "    # Scale similarities from [-1,1] to [0,1] range\n",
    "    scaled_similarities = (similarities + 1) / 2\n",
    "\n",
    "    # Print results\n",
    "    for i, (text_a, text_b, true_label) in enumerate(toy_data):\n",
    "        print(f\"\\nPair {i+1}:\")\n",
    "        print(f\"Text A: {text_a}\")\n",
    "        print(f\"Text B: {text_b}\")\n",
    "        print(f\"True Label: {true_label}\")\n",
    "        print(f\"Similarity Score: {scaled_similarities[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb790a2-e6de-4e24-a1f3-399b773b807b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Historical AV with SBERT Project Specific Kernel",
   "language": "python",
   "name": "historical_av_with_sbert-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
