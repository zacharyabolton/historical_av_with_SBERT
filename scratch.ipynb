{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4ecade1-ecc1-4fa4-a3e7-6ab4f84f21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def run_parallel(dataset, k_folds, k_td_values, devices):\n",
    "    results = []\n",
    "    with Pool(len(devices)) as pool:\n",
    "        # Create verifier instances for each device\n",
    "        tasks = [(k_td, fold, device) for k_td in k_td_values \n",
    "                                    for fold in range(k_folds)\n",
    "                                    for device in devices]\n",
    "        results = pool.starmap(defs.train_worker, tasks)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7440379-f9ad-4c60-9e9a-0b962724fa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fold': 0,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 0,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 0,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 1,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 1,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 1,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 2,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 2,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 2,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 0,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 0,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 0,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 1,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 1,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 1,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 2,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 2,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 2,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 0,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 0,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 0,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 1,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 1,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 1,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 2,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 2,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'},\n",
       " {'fold': 2,\n",
       "  'k_td': 0.5,\n",
       "  'results': '...need to implement main training logic...'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import defs\n",
    "run_parallel('foo', 3, [0, 0.5, 1], [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb7645c-d513-4bd3-bf5c-b09e7efa0637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential took: 7.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel took: 13.23 seconds\n",
      "Speedup factor: 0.54x\n"
     ]
    }
   ],
   "source": [
    "import defs\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "# Test sequential\n",
    "start_time = time.time()\n",
    "results_seq = [defs.train_worker_2(i) for i in range(3)]\n",
    "seq_time = time.time() - start_time\n",
    "print(f\"Sequential took: {seq_time:.2f} seconds\")\n",
    "\n",
    "# Test parallel\n",
    "start_time = time.time()\n",
    "with Pool(3) as p:\n",
    "    results_par = p.map(defs.train_worker_2, range(3))\n",
    "par_time = time.time() - start_time\n",
    "print(f\"Parallel took: {par_time:.2f} seconds\")\n",
    "\n",
    "print(f\"Speedup factor: {seq_time/par_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee63331-f452-4a07-b61e-86c1160f766b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sequential training...\n",
      "{'train_runtime': 6.4204, 'train_samples_per_second': 77.876, 'train_steps_per_second': 4.984, 'train_loss': 0.3266555666923523, 'epoch': 1.0}\n",
      "{'train_runtime': 6.3789, 'train_samples_per_second': 78.384, 'train_steps_per_second': 5.017, 'train_loss': 0.31156042218208313, 'epoch': 1.0}\n",
      "{'train_runtime': 5.8604, 'train_samples_per_second': 85.319, 'train_steps_per_second': 5.46, 'train_loss': 0.31156042218208313, 'epoch': 1.0}\n",
      "Sequential took: 25.23 seconds\n",
      "\n",
      "Running parallel training...\n",
      "Parallel took: 23.25 seconds\n",
      "Speedup factor: 1.09x\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import defs\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def run_benchmark():\n",
    "    # Test sequential\n",
    "    print(\"Running sequential training...\")\n",
    "    start_time = time.time()\n",
    "    results = [defs.train_fold(i) for i in range(3)]\n",
    "    seq_time = time.time() - start_time\n",
    "    print(f\"Sequential took: {seq_time:.2f} seconds\")\n",
    "    \n",
    "    # Test parallel\n",
    "    print(\"\\nRunning parallel training...\")\n",
    "    start_time = time.time()\n",
    "    with Pool(3) as p:\n",
    "        results = p.map(defs.train_fold, range(3))\n",
    "    par_time = time.time() - start_time\n",
    "    print(f\"Parallel took: {par_time:.2f} seconds\")\n",
    "    print(f\"Speedup factor: {seq_time/par_time:.2f}x\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0920145a-2889-46d5-8b1e-044b617b1c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3bffc3739842b1a898aafa7058786f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bdd09486ae4a7e950bdea8ef543b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef96082063834de2b1036f73fbaa962a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624b195b29e341fcb131100479135c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e694d3acb544ed6a0f2fcdbd5fec46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027ba96d16e14e768339e0d6e6e94fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca693020b9634c85ac006a7d664b53e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5653a4e3b4742d787b6dcefd1055a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c7ece0c6b346b48ac9774b0570be95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d7d2218abe4632a6a9c62b79d420e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f0789beeee44ac8884b0cc42a41bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384)\n",
      "tensor([[1.0000, 0.6660, 0.1046],\n",
      "        [0.6660, 1.0000, 0.1411],\n",
      "        [0.1046, 0.1411, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# @inproceedings{reimers-2019-sentence-bert,\n",
    "#   title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n",
    "#   author = \"Reimers, Nils and Gurevych, Iryna\",\n",
    "#   booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n",
    "#   month = \"11\",\n",
    "#   year = \"2019\",\n",
    "#   publisher = \"Association for Computational Linguistics\",\n",
    "#   url = \"https://arxiv.org/abs/1908.10084\",\n",
    "# }\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "# [3, 384]\n",
    "\n",
    "# 3. Calculate the embedding similarities\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)\n",
    "# tensor([[1.0000, 0.6660, 0.1046],\n",
    "#         [0.6660, 1.0000, 0.1411],\n",
    "#         [0.1046, 0.1411, 1.0000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f96d25-af15-4260-9c3d-5156eb66c29d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Historical AV with SBERT Project Specific Kernel",
   "language": "python",
   "name": "historical_av_with_sbert-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
